# -*- coding: utf-8 -*-
"""Named_Entity_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ardsEnutBFM1BH0jCBMNV2Wj7ohYqM6U

## Downloading and Preprocessing the Data
"""

from collections import defaultdict
from urllib import request
import json
import pandas as pd

def parse_conllu_using_pandas(block):
    records = []
    for line in block.splitlines():
        if not line.startswith('#'):
            records.append(line.strip().split('\t'))
    return pd.DataFrame.from_records(
        records,
        columns=['ID', 'FORM', 'TAG', 'Misc1', 'Misc2'])

def tokens_to_labels(df):
    return (
        df.FORM.tolist(),
        df.TAG.tolist()
    )

def simplified_tokens_to_labels(df):
    simplified_form_list = []
    simplified_tag_list = []
    simplified_form_list, tag_list = tokens_to_labels(df)
    for tag in tag_list:
        if tag.startswith('B-'):
            simplified_tag_list.append('B')
        elif tag.startswith('I-'):
            simplified_tag_list.append('I')
        elif tag == 'O':
            simplified_tag_list.append('O')
        else:
            raise ValueError('Unexpected Label')
    return (
        simplified_form_list,
        simplified_tag_list
    )

PREFIX = "https://raw.githubusercontent.com/UniversalNER/"
DATA_URLS = {
    "en_ewt": {
        "train": "UNER_English-EWT/master/en_ewt-ud-train.iob2",
        "dev": "UNER_English-EWT/master/en_ewt-ud-dev.iob2",
        "test": "UNER_English-EWT/master/en_ewt-ud-test.iob2"
    },
    "en_pud": {
        "test": "UNER_English-PUD/master/en_pud-ud-test.iob2"
    }
}

# en_ewt is the main train-dev-test split
# en_pud is the OOD test set
data_dict = defaultdict(dict)
for corpus, split_dict in DATA_URLS.items():
    for split, url_suffix in split_dict.items():
        url = PREFIX + url_suffix
        with request.urlopen(url) as response:
            txt = response.read().decode('utf-8')
            data_frames = map(parse_conllu_using_pandas,
                              txt.strip().split('\n\n'))
            token_label_alignments = list(map(tokens_to_labels,
                                              data_frames))
            data_dict[corpus][split] = token_label_alignments

simplified_data_dict = defaultdict(dict)
for corpus, split_dict in DATA_URLS.items():
    for split, url_suffix in split_dict.items():
        url = PREFIX + url_suffix
        with request.urlopen(url) as response:
            txt = response.read().decode('utf-8')
            data_frames = map(parse_conllu_using_pandas,
                              txt.strip().split('\n\n'))
            simplified_token_label_alignments = list(map(simplified_tokens_to_labels,
                                              data_frames))
            simplified_data_dict[corpus][split] = simplified_token_label_alignments

# Save the data
with open('ner_data_dict.json', 'w', encoding='utf-8') as out:
    json.dump(data_dict, out, indent=2, ensure_ascii=False)

with open('ner_simplified_data_dict.json', 'w', encoding = 'utf-8') as out:
    json.dump(simplified_data_dict, out, indent = 2, ensure_ascii = False)

# Each subset of each corpus is a list of tuples where each tuple
# is a list of tokens with a corresponding list of labels.

# Train on data_dict['en_ewt']['train']; validate on data_dict['en_ewt']['dev']
# and test on data_dict['en_ewt']['test'] and data_dict['en_pud']['test']
data_dict['en_ewt']['train'][0], data_dict['en_pud']['test'][1]

simplified_data_dict['en_ewt']['train'][0], simplified_data_dict['en_pud']['test'][1]

"""## Using BERT for Named Entity Recognition (NER)"""

from random import shuffle
from math import ceil

import torch
import torch.nn as nn

from transformers import AutoModel, AutoTokenizer
import datasets

from tqdm.auto import tqdm

"""We first fine-tune and evaluate the NER performance on original tagsets ('B-LOC', 'B-PER', 'B-ORG', 'I-LOC', 'I-PER', 'I-ORG', 'O')"""

class ClassificationHead(nn.Module):
    def __init__(self, model_dim = 768, n_classes = 7):
        super().__init__()
        self.linear = nn.Linear(model_dim, n_classes)

    def forward(self, x):
        return self.linear(x)

#A finetuned version (attempted but receive constant zero in the evaluation scores), so the single layer classification head is used in the experiment.
class ClassificationHead(nn.Module):
    def __init__(self, model_dim = 768, n_classes = 7):
        super().__init__()
        self.linear1 = nn.Linear(model_dim, 256)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(256, 128)
        self.linear3 = nn.Linear(128, n_classes)

    def forward(self, x):
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        return self.linear3(x)

labels = set()
for i in range(len(data_dict['en_ewt']['train'])):
    labels.update(data_dict['en_ewt']['train'][i][1])
n_classes = len(labels)
sorted(labels)

label_to_i = {
    label: i
    for i, label in enumerate(sorted(labels))
}
i_to_label = {
    i: label
    for label, i in label_to_i.items()
}

label_to_i

model_tag = 'google-bert/bert-base-uncased'

tokeniser = AutoTokenizer.from_pretrained(model_tag)

def process_sentence(sentence, label_to_i, tokeniser, encoder, clf_head,
                     encoder_device, clf_head_device):
    words, labels = sentence
    gold_labels = torch.tensor(
        [label_to_i[label] for label in labels]).to(clf_head_device)
    tokenisation = tokeniser(words, is_split_into_words=True,
                             return_tensors='pt')
    inputs = {k: v.to(encoder_device) for k, v in tokenisation.items()}

    # Remove the embedding of the CLS token and the SEP token.
    outputs = encoder(**inputs).last_hidden_state[0, 1:-1, :]

    # Take embeddings of the first/last subword and ignore the CLS and the SEP tokens
    word_ids = tokenisation.word_ids()[1:-1]
    processed_words = set()
    first_subword_embeddings = []

    for i, word_id in enumerate(word_ids):
        if word_id not in processed_words:
            first_subword_embeddings.append(outputs[i])
            processed_words.add(word_id)

    # Check the alignment of words and labels.
    assert len(first_subword_embeddings) == gold_labels.size(0)

    # Combine subword embeddings into a tensor and copy to the device of the classification head
    clf_head_inputs = torch.vstack(
        first_subword_embeddings).to(clf_head_device)

    # Return the logits and gold labels for subsequent processing
    return clf_head(clf_head_inputs), gold_labels

def train_epoch(data, label_to_i, tokeniser, encoder, clf_head,
                encoder_device, clf_head_device, loss_fn, optimiser):
    encoder.train()
    epoch_losses = torch.empty(len(data))
    for step_n, sentence in tqdm(
        enumerate(data),
        total=len(data),
        desc='Train',
        leave=False
    ):
        optimiser.zero_grad()
        logits, gold_labels = process_sentence(
            sentence, label_to_i, tokeniser,
            encoder, clf_head, encoder_device,
            clf_head_device)
        loss = loss_fn(logits, gold_labels)
        loss.backward()
        optimiser.step()
        epoch_losses[step_n] = loss.item()
    return epoch_losses.mean().item()

def extract_spans(labels):
    spans = []
    start = None # Start index of the span
    current_label = None

    # The span is defined by the boundary and the type of a named entity.
    # We extract the span in the format of (start_index, end_index, type)
    for i, tag in enumerate(labels):
        if tag.startswith('B-'):
            # Close previous span if open
            if start is not None:
                spans.append((start, i-1, current_label))
            start = i
            # Extract the type following the boundary
            current_label = tag[2:]
        elif tag.startswith('I-') and current_label == tag[2:]:
            # Continue current span if the type matches
            continue
        else:
            # Close previous span if open
            if start is not None:
                spans.append((start, i-1, current_label))
                start = None
                current_label = None

    # Close any open span at end
    if start is not None:
        spans.append((start, len(labels)-1, current_label))

    return spans

# Calculate the precision, recall, and f1 score using true positives, false positives, and false negatives
def scores(tp, fp, fn):
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    return precision, recall, f1_score


def span_matches(pred_span, gold_span, labelled=True):
    # Check both the boundary and type for labelled span matching scores
    if labelled:
        return pred_span == gold_span
    else:
        return pred_span[:2] == gold_span[:2]  # Check the boundary ('B', 'I', 'O') only


def validate_epoch(data, label_to_i, tokeniser, encoder, clf_head,
                   encoder_device, clf_head_device):
    encoder.eval()
    clf_head.eval()

    labelled_tp = 0
    labelled_fp = 0
    labelled_fn = 0

    unlabelled_tp = 0
    unlabelled_fp = 0
    unlabelled_fn = 0

    for step_n, sentence in tqdm(enumerate(data), total=len(data), desc='Eval', leave=False):
        with torch.no_grad():
            logits, gold_label_ids = process_sentence(
                sentence, label_to_i, tokeniser,
                encoder, clf_head, encoder_device,
                clf_head_device)

        predicted_label_ids = torch.argmax(logits, dim=-1).cpu().tolist()
        gold_label_ids = gold_label_ids.cpu().tolist()

        predicted_labels = [i_to_label[i] for i in predicted_label_ids]
        gold_labels = [i_to_label[i] for i in gold_label_ids]

        pred_spans = extract_spans(predicted_labels)
        gold_spans = extract_spans(gold_labels)

        # Labelled span metrics
        for pred_span in pred_spans:
            if any(span_matches(pred_span, gs, labelled=True) for gs in gold_spans):
                labelled_tp += 1
            else:
                labelled_fp += 1

        for gold_span in gold_spans:
            if not any(span_matches(gs, gold_span, labelled=True) for gs in pred_spans):
                labelled_fn += 1

        # Unlabelled span metrics
        for pred_span in pred_spans:
            if any(span_matches(pred_span, gs, labelled=False) for gs in gold_spans):
                unlabelled_tp += 1
            else:
                unlabelled_fp += 1

        for gold_span in gold_spans:
            if not any(span_matches(gs, gold_span, labelled=False) for gs in pred_spans):
                unlabelled_fn += 1

    labelled_precision, labelled_recall, labelled_f1 = scores(
        labelled_tp, labelled_fp, labelled_fn)

    unlabelled_precision, unlabelled_recall, unlabelled_f1 = scores(
        unlabelled_tp, unlabelled_fp, unlabelled_fn)

    return {
        "labelled span matching score": {
            "precision": labelled_precision,
            "recall": labelled_recall,
            "f1_score": labelled_f1,
        },
        "unlabelled span matching score": {
            "precision": unlabelled_precision,
            "recall": unlabelled_recall,
            "f1_score": unlabelled_f1,
        }
    }

encoder_device = 0
encoder = AutoModel.from_pretrained(
    model_tag).to(encoder_device)
clf_head = ClassificationHead(n_classes= n_classes)
clf_head_device = 0
clf_head.to(clf_head_device);

# baseline
n_epochs = 5
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    results = validate_epoch(data_dict['en_ewt']['dev'][:250], label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# fine-tuned validation results
n_epochs = 5
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    loss = train_epoch(data_dict['en_ewt']['train'][:500], label_to_i, tokeniser, encoder, clf_head,
                       encoder_device, clf_head_device, loss_fn, optimiser)
    print(f'Epoch {epoch_n+1} training loss: {loss:.2f}')
    results = validate_epoch(data_dict['en_ewt']['dev'][:250], label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# baseline test results
n_epochs = 5
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    results = validate_epoch(data_dict['en_ewt']['test'][:250], label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# baseline test results
n_epochs = 5
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    results = validate_epoch(data_dict['en_pud']['test'][:250], label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# fine-tuned test results
n_epochs = 5
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    loss = train_epoch(data_dict['en_ewt']['train'][:500], label_to_i, tokeniser, encoder, clf_head,
                       encoder_device, clf_head_device, loss_fn, optimiser)
    print(f'Epoch {epoch_n+1} training loss: {loss:.2f}')
    results = validate_epoch(data_dict['en_ewt']['test'][:250], label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# fine-tuned test results
n_epochs = 5
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    loss = train_epoch(data_dict['en_ewt']['train'][:500], label_to_i, tokeniser, encoder, clf_head,
                       encoder_device, clf_head_device, loss_fn, optimiser)
    print(f'Epoch {epoch_n+1} training loss: {loss:.2f}')
    results = validate_epoch(data_dict['en_pud']['test'][:250], label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

"""Then we switch to the simplified tagset ('B', 'I', 'O')"""

class ClassificationHead(nn.Module):
    def __init__(self, model_dim = 768, n_classes = 3):
        super().__init__()
        self.linear = nn.Linear(model_dim, n_classes)

    def forward(self, x):
        return self.linear(x)

simplified_labels = set()
for i in range(len(simplified_data_dict['en_ewt']['train'])):
    simplified_labels.update(simplified_data_dict['en_ewt']['train'][i][1])
n_simplified_classes = len(simplified_labels)
sorted(simplified_labels)

simplified_label_to_i = {
    simplified_label: i
    for i, simplified_label in enumerate(sorted(simplified_labels))
}
simplified_i_to_label = {
    i: simplified_label
    for simplified_label, i in simplified_label_to_i.items()
}

simplified_label_to_i

def process_sentence(sentence, label_to_i, tokeniser, encoder, clf_head,
                     encoder_device, clf_head_device):
    words, labels = sentence
    gold_labels = torch.tensor(
        [simplified_label_to_i[label] for label in labels]).to(clf_head_device)
    tokenisation = tokeniser(words, is_split_into_words=True,
                             return_tensors='pt')
    inputs = {k: v.to(encoder_device) for k, v in tokenisation.items()}

    # Remove the embedding of the CLS token and the SEP token.
    outputs = encoder(**inputs).last_hidden_state[0, 1:-1, :]

    # Take embeddings of the first/last subword and ignore the CLS and the SEP tokens
    word_ids = tokenisation.word_ids()[1:-1]
    processed_words = set()
    first_subword_embeddings = []

    for i, word_id in enumerate(word_ids):
        if word_id not in processed_words:
            first_subword_embeddings.append(outputs[i])
            processed_words.add(word_id)

    # Check the alignment of words and labels.
    assert len(first_subword_embeddings) == gold_labels.size(0)

    # Combine subword embeddings into a tensor and copy to the device of the classification head
    clf_head_inputs = torch.vstack(
        first_subword_embeddings).to(clf_head_device)

    # Return the logits and gold labels for subsequent processing
    return clf_head(clf_head_inputs), gold_labels

def train_epoch(data, label_to_i, tokeniser, encoder, clf_head,
                encoder_device, clf_head_device, loss_fn, optimiser):
    encoder.train()
    epoch_losses = torch.empty(len(data))
    for step_n, sentence in tqdm(
        enumerate(data),
        total=len(data),
        desc='Train',
        leave=False
    ):
        optimiser.zero_grad()
        logits, gold_labels = process_sentence(
            sentence, simplified_label_to_i, tokeniser,
            encoder, clf_head, encoder_device,
            clf_head_device)
        loss = loss_fn(logits, gold_labels)
        loss.backward()
        optimiser.step()
        epoch_losses[step_n] = loss.item()
    return epoch_losses.mean().item()

def extract_spans(labels):
    spans = []
    start = None # Start index of the span
    current_label = None

    # The span is defined by the boundary and the type of a named entity.
    # We extract the span in the format of (start_index, end_index, type)
    for i, tag in enumerate(labels):
        if tag.startswith('B-'):
            # Close previous span if open
            if start is not None:
                spans.append((start, i-1, current_label))
            start = i
            # Extract the type following the boundary
            current_label = tag[2:]
        elif tag.startswith('I-') and current_label == tag[2:]:
            # Continue current span if the type matches
            continue
        else:
            # Close previous span if open
            if start is not None:
                spans.append((start, i-1, current_label))
                start = None
                current_label = None

    # Close any open span at end
    if start is not None:
        spans.append((start, len(labels)-1, current_label))

    return spans

# Calculate the precision, recall, and f1 score using true positives, false positives, and false negatives
def scores(tp, fp, fn):
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    return precision, recall, f1_score


def span_matches(pred_span, gold_span, labelled=True):
    # Check both the boundary and type for labelled span matching scores
    if labelled:
        return pred_span == gold_span
    else:
        return pred_span[:2] == gold_span[:2]  # Check the boundary ('B', 'I', 'O') only


def validate_epoch(data, label_to_i, tokeniser, encoder, clf_head,
                   encoder_device, clf_head_device):
    encoder.eval()
    clf_head.eval()

    labelled_tp = 0
    labelled_fp = 0
    labelled_fn = 0

    unlabelled_tp = 0
    unlabelled_fp = 0
    unlabelled_fn = 0

    for step_n, sentence in tqdm(enumerate(data), total=len(data), desc='Eval', leave=False):
        with torch.no_grad():
            logits, gold_label_ids = process_sentence(
                sentence, label_to_i, tokeniser,
                encoder, clf_head, encoder_device,
                clf_head_device)

        predicted_label_ids = torch.argmax(logits, dim=-1).cpu().tolist()
        gold_label_ids = gold_label_ids.cpu().tolist()

        predicted_labels = [i_to_label[i] for i in predicted_label_ids]
        gold_labels = [i_to_label[i] for i in gold_label_ids]

        pred_spans = extract_spans(predicted_labels)
        gold_spans = extract_spans(gold_labels)

        # Labelled span metrics
        for pred_span in pred_spans:
            if any(span_matches(pred_span, gs, labelled=True) for gs in gold_spans):
                labelled_tp += 1
            else:
                labelled_fp += 1

        for gold_span in gold_spans:
            if not any(span_matches(gs, gold_span, labelled=True) for gs in pred_spans):
                labelled_fn += 1

        # Unlabelled span metrics
        for pred_span in pred_spans:
            if any(span_matches(pred_span, gs, labelled=False) for gs in gold_spans):
                unlabelled_tp += 1
            else:
                unlabelled_fp += 1

        for gold_span in gold_spans:
            if not any(span_matches(gs, gold_span, labelled=False) for gs in pred_spans):
                unlabelled_fn += 1

    labelled_precision, labelled_recall, labelled_f1 = scores(
        labelled_tp, labelled_fp, labelled_fn)

    unlabelled_precision, unlabelled_recall, unlabelled_f1 = scores(
        unlabelled_tp, unlabelled_fp, unlabelled_fn)

    return {
        "labelled span matching score": {
            "precision": labelled_precision,
            "recall": labelled_recall,
            "f1_score": labelled_f1,
        },
        "unlabelled span matching score": {
            "precision": unlabelled_precision,
            "recall": unlabelled_recall,
            "f1_score": unlabelled_f1,
        }
    }

encoder_device = 0
encoder = AutoModel.from_pretrained(
    model_tag).to(encoder_device)
clf_head = ClassificationHead(n_classes= n_classes)
clf_head_device = 0
clf_head.to(clf_head_device);

# baseline test results
n_epochs = 5
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    results = validate_epoch(simplified_data_dict['en_ewt']['test'][:250], simplified_label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# baseline test results
n_epochs = 5
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    results = validate_epoch(simplified_data_dict['en_pud']['test'][:250], simplified_label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# fine-tuned test results
n_epochs = 5
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    loss = train_epoch(simplified_data_dict['en_ewt']['train'][:500], simplified_label_to_i, tokeniser, encoder, clf_head,
                       encoder_device, clf_head_device, loss_fn, optimiser)
    print(f'Epoch {epoch_n+1} training loss: {loss:.2f}')
    results = validate_epoch(simplified_data_dict['en_ewt']['test'][:250], simplified_label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

# fine-tuned test results
n_epochs = 5
loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))
for epoch_n in tqdm(range(n_epochs)):
    loss = train_epoch(simplified_data_dict['en_ewt']['train'][:500], simplified_label_to_i, tokeniser, encoder, clf_head,
                       encoder_device, clf_head_device, loss_fn, optimiser)
    print(f'Epoch {epoch_n+1} training loss: {loss:.2f}')
    results = validate_epoch(simplified_data_dict['en_pud']['test'][:250], simplified_label_to_i, tokeniser, encoder,
                              clf_head, encoder_device, clf_head_device)
    print(results)

"""Next we will fine-tune T5 on NER."""

from random import shuffle
from math import ceil

import torch
import torch.nn as nn

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

from tqdm.auto import tqdm

import json

with open('ner_data_dict.json', 'r', encoding='utf-8') as f:
    data_dict = json.load(f)
with open('ner_simplified_data_dict.json', 'r', encoding = 'utf-8') as f:
    simplified_data_dict = json.load(f)

model_tag = 'google-t5/t5-base'

device = 0 if torch.cuda.is_available() else 'cpu'
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_tag, cache_dir='./hf_cache').to(device)
tokeniser = AutoTokenizer.from_pretrained(model_tag)

optim = torch.optim.AdamW(
    model.parameters(),
    lr=10**(-4))

labels = set()
for i in range(len(data_dict['en_ewt']['train'])):
    labels.update(data_dict['en_ewt']['train'][i][1])
n_classes = len(labels)
sorted(labels)

simplified_labels = set()
for i in range(len(simplified_data_dict['en_ewt']['train'])):
    simplified_labels.update(simplified_data_dict['en_ewt']['train'][i][1])
n_simplified_classes = len(simplified_labels)
sorted(simplified_labels)

def process_batch(batch_inputs, batch_labels,
                  tokeniser, model, device,
                  optimiser, max_len=512):
    optimiser.zero_grad()
    tokenisation = tokeniser(
        batch_inputs,
        return_tensors='pt',
        max_length=max_len,
        padding='longest',
        truncation=True
    )
    input_ids = tokenisation.input_ids.to(device)
    attention_mask = tokenisation.attention_mask.to(device)
    labels = tokeniser(
        batch_labels,
        return_tensors='pt',
        max_length=max_len,
        padding='longest',
        truncation=True
    ).input_ids.to(device)

    labels[labels == tokeniser.pad_token_id] = -100
    inputs = {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }
    loss = model(**inputs).loss
    loss.backward()
    optimiser.step()
    return loss.item()

def prepare_sentence(sentence_array):
    words, labels = sentence_array
    prepared_inputs = []
    for i in range(len(words)):
        tmp = words[:i] + ['~', words[i], '~'] + words[i+1:]
        prepared_inputs.append(' '.join(tmp))
    return prepared_inputs, labels

prepare_sentence(data_dict['en_ewt']['train'][0])

def train_epoch(train_inputs, batch_size,
                tokeniser, model, device, optimizer):
    model.train()

    n_steps = len(train_inputs)
    epoch_losses = torch.zeros(n_steps)
    for step_n in tqdm(range(n_steps), leave=False, desc='Train'):
        prepared_inputs, labels = prepare_sentence(train_inputs[step_n])
        n_batches = ceil(len(prepared_inputs) / batch_size)
        sentence_losses_accum = 0.0
        for step_n in range(n_batches):
            lo = step_n * batch_size
            hi = lo + batch_size
            batch_texts = prepared_inputs[lo:hi]
            batch_labels = labels[lo:hi]
            loss = process_batch(batch_texts, batch_labels,
                                 tokeniser, model, device,
                                 optimizer)
            sentence_losses_accum += loss
        epoch_losses[step_n] = sentence_losses_accum / n_batches
    return epoch_losses.mean().item()

def get_class_prediction(prompt, tokeniser, model, device, max_len=512):
    tokenisation = tokeniser(
        prompt,
        return_tensors='pt',
        max_length=max_len,
        truncation=True
    )
    input_ids = tokenisation.input_ids.to(device)
    attention_mask = tokenisation.attention_mask.to(device)
    output = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
    max_new_tokens=10).squeeze()
    output_string = tokeniser.decode(
        output,
        skip_special_tokens=True
    ).strip()
    if not output_string:
        return None
    return output_string.split()[0]

def validate_epoch(dev_inputs, tokeniser, model, device, max_len=512):
    model.eval()
    n_steps = len(dev_inputs)
    epoch_hits = []
    for step_n in tqdm(range(n_steps), leave=False, desc='Validate'):
        prepared_inputs, labels = prepare_sentence(dev_inputs[step_n])
        with torch.no_grad():
            for input_sentence, gold_label in zip(prepared_inputs, labels):
                predicted_label = get_class_prediction(
                    input_sentence, tokeniser, model, device,
                    max_len=max_len)
                epoch_hits.append(int(predicted_label == gold_label))
    return sum(epoch_hits) / len(epoch_hits)

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_dev_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_loss = train_epoch(data_dict['en_ewt']['train'][: n_train_exx], batch_size,
                             tokeniser, model, device, optim)
    print(f'Epoch {epoch_n+1} loss:', round(epoch_loss, 2))
    epoch_dev_accuracy = validate_epoch(
        data_dict['en_ewt']['dev'][: n_dev_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} dev accuracy: {epoch_dev_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_test_accuracy = validate_epoch(
        data_dict['en_ewt']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_test_accuracy = validate_epoch(
        data_dict['en_pud']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_loss = train_epoch(data_dict['en_ewt']['train'][: n_train_exx], batch_size,
                             tokeniser, model, device, optim)
    print(f'Epoch {epoch_n+1} loss:', round(epoch_loss, 2))
    epoch_test_accuracy = validate_epoch(
        data_dict['en_ewt']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_loss = train_epoch(data_dict['en_ewt']['train'][: n_train_exx], batch_size,
                             tokeniser, model, device, optim)
    print(f'Epoch {epoch_n+1} loss:', round(epoch_loss, 2))
    epoch_test_accuracy = validate_epoch(
        data_dict['en_pud']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_test_accuracy = validate_epoch(
        simplified_data_dict['en_ewt']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_test_accuracy = validate_epoch(
        simplified_data_dict['en_pud']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_loss = train_epoch(simplified_data_dict['en_ewt']['train'][: n_train_exx], batch_size,
                             tokeniser, model, device, optim)
    print(f'Epoch {epoch_n+1} loss:', round(epoch_loss, 2))
    epoch_test_accuracy = validate_epoch(
        simplified_data_dict['en_ewt']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')

n_epochs = 3
batch_size = 16

n_train_exx = 500
n_test_exx = 100

best_accuracy = 0.0
for epoch_n in tqdm(range(n_epochs)):
    epoch_loss = train_epoch(simplified_data_dict['en_ewt']['train'][: n_train_exx], batch_size,
                             tokeniser, model, device, optim)
    print(f'Epoch {epoch_n+1} loss:', round(epoch_loss, 2))
    epoch_test_accuracy = validate_epoch(
        simplified_data_dict['en_ewt']['test'][: n_test_exx], tokeniser, model, device)

    print(f'Epoch {epoch_n+1} test accuracy: {epoch_test_accuracy:.2f}')